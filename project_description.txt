	Not everyone who speaks the same language speaks it in the same way. In England, folks are currently passing the COVID-19 crisis at home in their ‘pants’ getting ‘pissed’ while here in the U.S. we are in our ‘undies’ getting ‘wasted’. Dialectal differences like this can pose a challenge to both humans and machines, especially when it comes to processing speech with different accents. 
	One piece of knowledge that could make accent detection more tractable is what I call dialect detection. That is, detecting the individual words (‘pissed’ vs. ‘wasted’) that characterize a particular dialect, as opposed to the pronunciations of those words. Take a simple illustration: if you pick up the telephone and hear a foreign accent, it might be difficult to understand. But if the phone rings and you notice that the call is coming from Australia, you might go into the call prepared to rely on whatever past experiences with that accent you have had, and the call might go more smoothly. I other words, the top-down information about which dialect you are about to hear primes you to better process the bottom-up information of the accent you are about to hear. 
	This project takes one step towards identifying dialects in Spanish. In this initial analysis, I explore how useful it is to use (i) subject pronouns, and (ii) specific verbs and nouns to distinguish between dialects. 
	For the larger project, I propose to build a classifier that can identify someone’s dialect, given an utterance of reasonable length. The exploratory analysis done here shows that useful features for this classifier to use may include: (i) types and overall rates of subject pronoun use, and (ii) ratios between words that mean similar things. 
	I use a dataset compiled from free-speech corpora publicly available from https://childes.talkbank.org/access/Spanish/, supplemented with two corpora from my own research. The dialects represented in this dataset include (i) Mexican, (ii) Argentinian, (iii) Paraguayan, and (iv) Spain Spanish. Each corpus contains at least 20 text transcripts of children and parents interacting in naturalistic settings (playing, storytelling, games). Each utterance has been tagged for speaker and the words have been passed through the part-of-speech tagger designed for Spanish by Brian MacWhinney and described here: https://talkbank.org/manuals/MOR.pdf . An important feature of this tagger is that it identifies verb stems (ex. esta-) so that different inflections of the same verb (ex. estoy, ‘(I) am’, estás ‘(you) are’, etc.) can be easily grouped together.
	I chose to use free-speech data, since this is the kind of speech that systems like Alexa need to understand. In this project, I will only be analyzing adult speech, but future projects can take advantage of children’s speech to answer clinical questions (ex. “Is this child using the kind of speech appropriate for their age?”) or general research questions (ex. “For children exposed to multiple dialects, what proportion of their utterances can be classified as dialect 1 vs. dialect 2?”). 
	For the classifier I will actually use Python instead of R, in order to take advantage of Python's nltk toolkit. I am less comfortable in Python than R, but I have sat in on a course where we built classifiers in Python, and I will use the code from class as a jumping off point.

	